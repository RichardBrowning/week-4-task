{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the week 3 task for the Buckeye AutoDrive team\n",
    "# This week, you will be learning the data representation from the various sensors on the car\n",
    "# Your task is to read through the sample codes/comments I provided and finish the tasks specified in the comments\n",
    "# After you finish the tasks, please zip the finished code along with the generated visualizations and post it on Teams, thanks!\n",
    "# Good luck! Reach out to me on Teams if you have any questions - Daniel Feng\n",
    "# Note: To run this code, you need to have Jupyter Notebook installed on your computer\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "\n",
    "# Load the data from the file\n",
    "image_dir = \"dataset_Mcity_1_Jun_8_23_KITTI_no_labels/image_2\"\n",
    "lidar_dir = \"dataset_Mcity_1_Jun_8_23_KITTI_no_labels/velodyne\"\n",
    "calib_path = \"dataset_Mcity_1_Jun_8_23_KITTI_no_labels/calib.json\"\n",
    "\n",
    "# code to load the calibration information from the file\n",
    "# calibration information is used to project the 3D point cloud to the camera image\n",
    "def read_calib_file(filepath):\n",
    "    \"\"\"\n",
    "    Read the calibration file and parse the data\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        calib = json.load(f)\n",
    "    return calib\n",
    "\n",
    "def visualize_pointcloud(pointcloud, color= None):\n",
    "    \"\"\"\n",
    "    Visualize the point cloud\n",
    "    \"\"\"\n",
    "    fig = go.Figure(layout=dict(scene=dict(aspectmode=\"data\")))\n",
    "    if color is not None:\n",
    "        # Set the colormap to viridis\n",
    "        fig.add_trace(go.Scatter3d(x=pointcloud[:, 0], y=pointcloud[:, 1], z=pointcloud[:, 2], mode='markers', marker=dict(size=1, color=color, colorscale='Plasma', opacity=0.8)))\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter3d(x=pointcloud[:, 0], y=pointcloud[:, 1], z=pointcloud[:, 2], mode='markers', marker=dict(size=1)))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List the image and lidar files, sort them by name. This will usually align corresponding images and lidar scans,\n",
    "# as the scans that are taken at the same time will usually share the same file name\n",
    "image_paths = sorted([os.path.join(image_dir, x) for x in os.listdir(image_dir)])\n",
    "lidar_paths = sorted([os.path.join(lidar_dir, x) for x in os.listdir(lidar_dir)])\n",
    "\n",
    "# Let's visualize the first image and the first lidar scan!\n",
    "# Load the image\n",
    "image = cv2.imread(image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Load the cepton lidar scan, it's in the form of a binary file,\n",
    "# storing numpy array with shape (N, 4), where N is the number of points\n",
    "lidar = np.fromfile(lidar_paths[0], dtype=np.float32).reshape(-1, 4)\n",
    "\n",
    "# visualize the point cloud\n",
    "fig = visualize_pointcloud(lidar[:, :3])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's play around with the image data!\n",
    "# Check the dimensions of the image first!\n",
    "print(\"Image shape:\", image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, the image is a 3D numpy array, with the first dimension being the height of the image,\n",
    "# the second dimension being the width of the image, and the third dimension being the color channels (RGB)\n",
    "# For more information, check this blog: https://developer.ibm.com/articles/learn-the-basics-of-computer-vision-and-object-detection/\n",
    "# Let's verify this representation!\n",
    "\n",
    "image_height, image_width, image_channels = image.shape\n",
    "\n",
    "# Now, let's crop the image width by half:\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "image_horizontal_crop = image[:, :image_width // 2, :] # here, we reduce the second dimension of the image data by half\n",
    "ax[0].imshow(image_horizontal_crop)\n",
    "image_horizontal_crop2 = image[:, image_width // 2:, :] # take the second half of the image\n",
    "ax[1].imshow(image_horizontal_crop2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 1: Let's crop the image height by half and visualize it!\n",
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the first two dimensions of the image verified, let's check the color channels!\n",
    "# We first separate the image into red, green, and blue channels\n",
    "red_channel = image[:, :, 0]\n",
    "green_channel = image[:, :, 1]\n",
    "blue_channel = image[:, :, 2]\n",
    "\n",
    "# Let's offset the red channel by 25 to the left, blue channel by 25 to the right, and visualize the result!\n",
    "plt.clf()\n",
    "red_channel_offset = np.roll(red_channel, -25, axis=1) # roll the red channel to the left by 25 pixels\n",
    "blue_channel_offset = np.roll(blue_channel, 25, axis=1)\n",
    "image_offset = np.stack([red_channel_offset, green_channel, blue_channel_offset], axis=-1) # stack the channels back together!\n",
    "plt.imshow(image_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty cool, right? Now let's introduce the concept of color space!\n",
    "# The RGB color space is the most common color space, but there are many others!\n",
    "# One of the most common color spaces we use in computer vision is the HSV color space\n",
    "# The HSV color space represents colors as Hue, Saturation, and Value\n",
    "# Let's convert the image to the HSV color space and visualize the channels!\n",
    "\n",
    "image_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "hue_channel = image_hsv[:, :, 0]\n",
    "saturation_channel = image_hsv[:, :, 1]\n",
    "value_channel = image_hsv[:, :, 2]\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# In the first subplot, we offset all pixel hue by 25 to the right and see what happens!\n",
    "hue_channel_offset = (hue_channel + 25) % 180 # we need to make sure the hue channel is within the range of 0 to 180\n",
    "image_hsv_offset = np.stack([hue_channel_offset, saturation_channel, value_channel], axis=-1)\n",
    "image_rgb_offset = cv2.cvtColor(image_hsv_offset, cv2.COLOR_HSV2RGB) # we always convert the image back to RGB before visualizing it\n",
    "ax[0].imshow(image_rgb_offset)\n",
    "\n",
    "# In the second subplot, we increase all pixel saturation by 100 to the left and see what happens!\n",
    "saturation_channel_offset = np.clip(saturation_channel + 100, 0, 255) # we need to make sure the saturation channel is within the range of 0 to 255\n",
    "image_hsv_offset = np.stack([hue_channel, saturation_channel_offset, value_channel], axis=-1)\n",
    "image_rgb_offset = cv2.cvtColor(image_hsv_offset, cv2.COLOR_HSV2RGB)\n",
    "ax[1].imshow(image_rgb_offset)\n",
    "\n",
    "# In the third subplot, we increase all pixel values by 2 times and see what happens!\n",
    "value_channel_offset = np.clip(value_channel.astype(np.int16) * 2, 0, 255).astype(np.uint8) # we need to make sure the value channel is within the range of 0 to 255\n",
    "image_hsv_offset = np.stack([hue_channel, saturation_channel, value_channel_offset], axis=-1)\n",
    "image_rgb_offset = cv2.cvtColor(image_hsv_offset, cv2.COLOR_HSV2RGB)\n",
    "ax[2].imshow(image_rgb_offset)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Task 2: Let's try remove all the bright pixels in the image and visualize it! (remove the sky pixels)\n",
    "# Hint: Which channel in what color space can help you identify brighter pixels?\n",
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With all that being said, let's use all the knowledge we learned to do something we can use in the real world!\n",
    "# Let's try to detect the traffic lights in the image!\n",
    "\n",
    "tl_image = cv2.imread(\"dataset_Mcity_1_Jun_8_23_KITTI_no_labels/image.png\")\n",
    "\n",
    "def traffic_light_detection(img, thresholds):\n",
    "    \"\"\"\n",
    "    Detect all active traffic lights from the image.\n",
    "\n",
    "    Args:\n",
    "        img (MatLike): cv2 image that contains the traffic light.\n",
    "        show_layers (bool): show the intermediate color filtering layers, for debugging purposes.\n",
    "\n",
    "    Returns:\n",
    "        [red_light, yellow_light, green_light],\n",
    "        where each (color)_light denotes a list of lights in corresponding\n",
    "        color in format [x, y, radius].\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # convert target img to hsv color space\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # apply bilateral filter to smooth out some textures\n",
    "    hsv = cv2.bilateralFilter(hsv, 9,75,75 )\n",
    "    \n",
    "    # separate red green yellow color channels and filter out target color\n",
    "    lower_red1, upper_red1, lower_red2, upper_red2, lower_green, upper_green, lower_yellow, upper_yellow = thresholds\n",
    "    maskr1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    maskr2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    maskg = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    masky = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "    maskr = cv2.add(maskr1, maskr2)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    red_bulbs = img.copy()\n",
    "    red_bulbs[maskr == 0] = 0\n",
    "    green_bulbs = img.copy()\n",
    "    green_bulbs[maskg == 0] = 0\n",
    "    yellow_bulbs = img.copy()\n",
    "    yellow_bulbs[masky == 0] = 0\n",
    "    return red_bulbs, yellow_bulbs, green_bulbs\n",
    "\n",
    "\n",
    "lower_red1 = np.array([0,30,100])\n",
    "upper_red1 = np.array([18,255,255])\n",
    "lower_red2 = np.array([170,30,200])\n",
    "upper_red2 = np.array([180,255,255])\n",
    "lower_green = np.array([50,60,150])\n",
    "upper_green = np.array([100,255,255])\n",
    "lower_yellow = np.array([20,115,150])\n",
    "upper_yellow = np.array([35,255,255])\n",
    "thresholds = [lower_red1, upper_red1, lower_red2, upper_red2, lower_green, upper_green, lower_yellow, upper_yellow]\n",
    "results = traffic_light_detection(tl_image, thresholds)\n",
    "\n",
    "# Let's visualize the results!\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "ax[0].imshow(cv2.cvtColor(tl_image, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[1].imshow(results[0])\n",
    "ax[1].set_title(\"Red Traffic Lights\")\n",
    "ax[2].imshow(results[1])\n",
    "ax[2].set_title(\"Yellow Traffic Lights\")\n",
    "ax[3].imshow(results[2])\n",
    "ax[3].set_title(\"Green Traffic Lights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Task 3: Can you explain how the traffic light detection algorithm works by looking at the code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's play around with the lidar data!\n",
    "# Check the dimensions of the lidar data first!\n",
    "print(\"Lidar shape:\", lidar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, our lidar data is a 2D numpy array, with the first dimension being the number of points,\n",
    "# and the second dimension being the number of features for each point\n",
    "# In this case, we have 4 features for each point: x, y, z, and reflectance (how much light is reflected back to the sensor)\n",
    "# Let's verify this representation!\n",
    "\n",
    "# visualize the point cloud, coloring by the x coordinate value (darker color means smaller x value)\n",
    "visualize_pointcloud(lidar[:, :3], color=lidar[:, 0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the point cloud, coloring by the y coordinate value (darker color means smaller y value)\n",
    "visualize_pointcloud(lidar[:, :3], color=lidar[:, 1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the point cloud, coloring by the z coordinate value (darker color means smaller z value)\n",
    "visualize_pointcloud(lidar[:, :3], color=lidar[:, 2]).show()\n",
    "# as you can see, higher points are colored in brighter colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Task 4: Let's try to filter out the points that are below the ground plane and visualize it!\n",
    "# Hint: It's not a good ground removal method, but it's a simple one\n",
    "# You can remove points that are below a certain height"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perception_sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
